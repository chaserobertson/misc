{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab1d1d6",
   "metadata": {},
   "source": [
    "# COMPSCI 762 Assignment 2 Naive Bayes Classifier\n",
    "Chase Robertson  \n",
    "crob873"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba9689",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "In order to match the benchmark model performance initially, some default parameters had to be modified straightaway. I chose to ignore prior probabilities in the Naive Bayes' probability calculation on suspicion that the training data's category distribution was imbalanced. I also chose to tune the model's smoothing parameter, as that seemed a necessary change to account for words with zero count. After making those two changes, I was able to closely match the benchmark model's performance. Further improvement to prediction scores then needed to be achieved by other means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834b073",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "I chose to first attempt to improve prediction by excluding english stop words from the word frequency vector. I also attempted to include bigrams in the vector, and tried turning off the default inverse document frequency calculation executed by the term frequency utility provided by `sklearn`. I also switched to the Rennie et al. (2003) version of Multinomial Naive Bayes model to try to address the issues illuminated in that paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ae34e3",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "I chose to add the `name` and `mean_checkin_time` attributes to the model to try to improve prediction scores. Some establishment names explicitly include class information, so the benefit of including that attribute is obvious. I chose to represent that attribute as a bag of words in the same or similar way as the `review` attribute. I suspected that information about the time spent checking in to the establishment may be helpful as well, as some categories may be associated with longer or shorter checkins. The only other attributes available in the dataset are latitude and longitude, which do not seem very informative since different types of businesses can be mixed in the same area. There could be some association between output class and location due to local tradition or zoning policy, but it would surely require some clever preprocessing for the Naive Bayes model to discover those associations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d4afb",
   "metadata": {},
   "source": [
    "## Data Representation and Preprocessing\n",
    "Each observation is initially represented by a term frequency vector, constructed from the `review` attribute. The default behavior of `sklearn`'s `TfidfTransformer` is to incorporate the inverse document frequency in each term frequency. In Task 1, that default behavior is turned off, so basic term frequency is used. English stop words are  removed from the term frequency vector in both Task 1 and 2. Bigrams from the `review` feature are unsuccessfully included in Task 1, but somewhat successfully included in Task 2, with the difference in effect probably due to the additional maximum limit placed on the number of features in Task 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c8112",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "Initial implementation focused on boilerplate setup and matching prediction performance with the benchmark level of about 87%. This was achieved by removing prior probabilities from prediction, and tuning the smoothing parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13584392",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Though many changes were attempted in Task 1, the real improvement in prediction came from the combination of removing english stop words and skipping the inverse document frequency calculation. These changes, in concert with the use of Rennie et al. (2003) model, led to a consistent prediction accuracy of almost 90%. I believe the removal of stop words and use of simpler term frequency vectors enabled the more robust `ComplementNB` model to resist bias to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d3803",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Adding the `name` and `mean_checkin_time` attributes in Task 2 alone led to a small improvement in prediction accuracy. When considering the addition of more term frequency features, it occurred to me that there may be thousands of features already in the model that were not very informative. Adding the `max_features` argument to the Naive Bayes model, in combination with the additional attributes and re-tuning of existing hyperparameters, led to a significant improvement over the benchmark model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd45dfcc",
   "metadata": {},
   "source": [
    "## Evaluation Procedure\n",
    "All models were evaluated and their hyperparameters selected by 5-fold cross-validation. I decided that cross-validation was necessary to maximize the number of observations available for training, since only a few thousand observations were available in total. Fewer cross-validation folds were used for intermediate exploration, as computation time of exhaustive grid search over many hyperparameters became an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb49cc9",
   "metadata": {},
   "source": [
    "## Validation Results\n",
    "The validation accuracy of each model is listed below. Again, this was calculated using 5-fold cross-validation to maximize the exposure of each model to the small training set.\n",
    "\n",
    "- Task 0 (benchmark): **88.5%**\n",
    "- Task 1 (optimised): **89.4%**\n",
    "- Task 2 (additional attributes): **91.4%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb8c0e",
   "metadata": {},
   "source": [
    "# Code\n",
    "## Part 0a - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990169d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "np.random.seed(12345678)\n",
    "\n",
    "PATH_ROOT = Path(os.getcwd())\n",
    "train_file = os.path.join(PATH_ROOT, 'train.csv')\n",
    "test_file = os.path.join(PATH_ROOT, 'test.csv')\n",
    "benchmark_file = os.path.join(PATH_ROOT, 'benchmark_predict.csv')\n",
    "improved_file = os.path.join(PATH_ROOT, 'improved_predict.csv')\n",
    "plus_attr_file = os.path.join(PATH_ROOT, 'plus_attr_predict.csv')\n",
    "\n",
    "train = pd.read_csv(train_file)\n",
    "X_train_review = train['review']\n",
    "y_train = train['category']\n",
    "test = pd.read_csv(test_file)\n",
    "X_test_review = test['review']\n",
    "X_test_id = test['ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae227404",
   "metadata": {},
   "source": [
    "## Part 0b - Replicate Benchmark Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecf9596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation score: 0.885144675049235\n",
      "----- Achieved with -------\n",
      "clf__alpha: 0.10000000000000003\n",
      "clf__fit_prior: False\n"
     ]
    }
   ],
   "source": [
    "# Replicate Benchmark Model\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "bench_params = {\n",
    "    'clf__alpha': np.arange(0.2, 0, -0.05),\n",
    "    'clf__fit_prior': [True, False],\n",
    "}\n",
    "\n",
    "bench_cv = GridSearchCV(pipe, bench_params, n_jobs=-1)\n",
    "bench_model = bench_cv.fit(X_train_review, y_train)\n",
    "\n",
    "print(\"Best validation score: %r\" % (bench_model.best_score_))\n",
    "print(\"----- Achieved with -------\")\n",
    "for name in sorted(bench_params.keys()):\n",
    "    print(\"%s: %r\" % (name, bench_model.best_params_[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f801b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predictions to file\n",
    "y_test_pred = pd.Series(bench_model.predict(X_test_review), \n",
    "                        name='category')\n",
    "submission = pd.concat([X_test_id, y_test_pred], axis=1)\n",
    "submission.to_csv(benchmark_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56faf604",
   "metadata": {},
   "source": [
    "## Part 1 - Improve based on Review only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23039999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement over benchmark: 0.008695046205120405\n",
      "Best validation score: 0.8938397212543554\n",
      "----- Achieved with -------\n",
      "clf__alpha: 0.1\n",
      "tfidf__use_idf: False\n",
      "vect__ngram_range: (1, 1)\n",
      "vect__stop_words: 'english'\n"
     ]
    }
   ],
   "source": [
    "# Improve Benchmark Model with complement model and new params\n",
    "improved_pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    # IMPROVEMENT use model from Rennie et al. 2003\n",
    "    ('clf', ComplementNB()),\n",
    "])\n",
    "\n",
    "improved_params = {\n",
    "    # IMPROVEMENT params \n",
    "    'vect__stop_words': ['english'],\n",
    "    'vect__ngram_range': [(1,1), (1,2)],\n",
    "    'tfidf__use_idf': [True, False],\n",
    "    # BENCHMARK params\n",
    "    'clf__alpha': [0.1],\n",
    "    # fit_prior unneccessary - only applies to edge cases in ComplementNB\n",
    "}\n",
    "\n",
    "improved_cv = GridSearchCV(improved_pipe, improved_params, cv=5, n_jobs=-1)\n",
    "improved_model = improved_cv.fit(X_train_review, y_train)\n",
    "\n",
    "print(\"Improvement over benchmark: %r\" %(improved_model.best_score_ - bench_model.best_score_))\n",
    "print(\"Best validation score: %r\" % (improved_model.best_score_))\n",
    "print(\"----- Achieved with -------\")\n",
    "for name in sorted(improved_params.keys()):\n",
    "    print(\"%s: %r\" % (name, improved_model.best_params_[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c95ee50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predictions to file\n",
    "y_test_pred = pd.Series(improved_model.predict(X_test_review), \n",
    "                        name='category')\n",
    "submission = pd.concat([X_test_id, y_test_pred], axis=1)\n",
    "submission.to_csv(improved_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbc3fcc",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f107b4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement over benchmark: 0.028885320405999004\n",
      "Improvement over Part 1: 0.0201902742008786\n",
      "Best validation score: 0.914029995455234\n",
      "----- Achieved with -------\n",
      "clf__alpha: 0.32\n",
      "clf__fit_prior: True\n",
      "pre__name__tfidf__use_idf: False\n",
      "pre__review__vect__max_features: 3500\n",
      "pre__review__vect__ngram_range: (1, 2)\n",
      "pre__review__vect__stop_words: 'english'\n"
     ]
    }
   ],
   "source": [
    "# Improve model with additional attributes\n",
    "\n",
    "# Add 'name' attr and use the same BoW preprocess as 'review'\n",
    "words_attrs = ['review', 'name']\n",
    "words_pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "])\n",
    "\n",
    "# Add mean checkin time without any preprocessing\n",
    "num_attrs = ['mean_checkin_time']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('review', words_pipe, 'review'),\n",
    "    ('name', words_pipe, 'name'),\n",
    "    ('num', 'passthrough', num_attrs),\n",
    "])\n",
    "\n",
    "best_pipe = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('clf', ComplementNB()),\n",
    "])\n",
    "\n",
    "best_params = {\n",
    "    # TASK 2 IMPROVEMENT params\n",
    "    'pre__review__vect__max_features': np.arange(3000, 5000, 500),\n",
    "    # TASK 1 IMPROVEMENT params\n",
    "    'pre__review__vect__stop_words': ['english'], # , None],\n",
    "    'pre__review__vect__ngram_range': [(1,2)], #, (1,1)],\n",
    "    #'pre__name__vect__ngram_range': [(1,1), (1,2)],\n",
    "    #'pre__review__tfidf__use_idf': [False], #, True],\n",
    "    'pre__name__tfidf__use_idf': [False], #, True],\n",
    "    # BENCHMARK params\n",
    "    'clf__alpha': np.arange(0.34, 0.25, -0.02), #np.arange(0.3, 0.22, -0.02),\n",
    "    'clf__fit_prior': [True]#, False],\n",
    "}\n",
    "\n",
    "best_cv = GridSearchCV(best_pipe, best_params, cv=5, n_jobs=-1)\n",
    "\n",
    "best_model = best_cv.fit(train, y_train)\n",
    "\n",
    "print(\"Improvement over benchmark: %r\" %(best_model.best_score_ - bench_model.best_score_))\n",
    "print(\"Improvement over Part 1: %r\" %(best_model.best_score_ - improved_model.best_score_))\n",
    "print(\"Best validation score: %r\" % (best_model.best_score_))\n",
    "print(\"----- Achieved with -------\")\n",
    "for name in sorted(best_params.keys()):\n",
    "    print(\"%s: %r\" % (name, best_model.best_params_[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9c7f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predictions to file\n",
    "y_test_pred = pd.Series(best_model.predict(test), \n",
    "                        name='category')\n",
    "submission = pd.concat([X_test_id, y_test_pred], axis=1)\n",
    "submission.to_csv(plus_attr_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
